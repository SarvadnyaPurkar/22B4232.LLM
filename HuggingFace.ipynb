{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnwZwIhBFqNdO8c1b0SP8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarvadnyaPurkar/22B4232.LLM/blob/main/HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xSGbIsEZpZma"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain-huggingface\n",
        "# !pip install huggingface_hub\n",
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install accelerate\n",
        "# !pip install torch\n",
        "# !pip install bitsandbytes\n",
        "# !pip install langchain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment secret keys\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get('HF_TOKEN')\n",
        "print(sec_key)\n",
        "# Using this sec_key we can call any api and donot need to download anything\n",
        "# on the local desktop\n",
        "# Create YOur Own key and use it"
      ],
      "metadata": {
        "id": "82zVLkqNp5_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5bed59-335e-4439-a773-58876c97cd1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "sec_key = userdata.get('HUGGINGFACETOKEN')\n",
        "print(sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8MgL1kdLUvk",
        "outputId": "b669d1f0-11f9-4d3d-dc1a-2745d7577601"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "fdt_MyhzIs_D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key"
      ],
      "metadata": {
        "id": "JMtnW1zMJ3Kx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id, max_length = 128,temperature = 0.7,token = sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl0bNxbhLskX",
        "outputId": "87bac9f0-423a-4e68-8d84-430001072636"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jygAzovwL42N",
        "outputId": "37342590-1e09-4bad-9982-314c8ddc6d25"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.7, model_kwargs={'max_length': 128, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What is machine learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "BM17JAzBMVNI",
        "outputId": "5d41f455-143b-49db-9adc-586e414e3694"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nMachine learning algorithms build a mathematical model based on sample data, known as “training data,” in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are designed to find patterns in data and make decisions based on those patterns.\\n\\nMachine learning is used in a variety of applications, including image recognition, speech recognition, and natural language processing. It is also used in predictive modeling, fraud detection, and recommendation systems.\\n\\nThere are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is trained on labeled data, meaning that the data includes both the input (features) and the desired output (target variable). The algorithm learns to map inputs to outputs based on the training data. In unsupervised learning, the algorithm is given unlabeled data and must find patterns and relationships in the data on its own. In reinforcement learning, the algorithm learns by interacting with its environment and receiving rewards or penalties based on its actions.\\n\\nMachine learning is a powerful tool that can be used to solve complex problems and make accurate predictions. It has the potential to revolutionize industries and improve our lives in many ways. However, it also raises important ethical and societal questions, such as issues of privacy, bias, and the impact on employment. As machine learning continues to evolve and become more prevalent, it will be important for individuals and organizations to consider these issues and use machine learning in a responsible and ethical way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id, max_length = 128,temperature = 0.7,token = sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jY9fafxMbDv",
        "outputId": "b1b73024-1a36-4ce2-d84f-52031c26df0b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"what is generative AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "cwLof-dQM51n",
        "outputId": "be1779f5-75c9-4eb7-b20c-f702d00146b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? Generative AI is a type of artificial intelligence that can create new content or data that resembles the structure and style of existing data. This is done by learning patterns and relationships in the data, and then using that knowledge to generate new content. Generative AI can be used in a variety of applications, such as generating images, music, text, and even 3D models. Some examples of generative AI include deep learning models like generative adversarial networks (GANs) and variational autoencoders (VAEs). These models are trained on large datasets and can generate new content that is similar to the training data, but also has some unique characteristics. Generative AI is a powerful tool for creating new and original content, and is being used in many fields, including art, entertainment, and technology.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "tjMGWj3BNB0-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who won 2011 cricket world cup\"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgcwS5a2NWpt",
        "outputId": "0a237ea3-f2c7-489e-a3df-4f0350ef6995"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['question'] template=\"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# providing answer to the given question\n",
        "# the llm chain is given a promp question which is then run to find the answer\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsfQKjeHOB9n",
        "outputId": "3c3db0eb-f49f-417f-d885-7734a8c4cd1f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The 2011 Cricket World Cup was held in India, Sri Lanka, and Bangladesh. The final match of the tournament took place on April 2, 2011, between India and Sri Lanka at the Wankhede Stadium in Mumbai, India.\n",
            "\n",
            "India won the match by six wickets, with Gautam Gambhir being named the Man of the Match for his outstanding performance. This was India's second Cricket World Cup victory, the first being in 1983.\n",
            "\n",
            "So, India won the 2011 Cricket World Cup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.invoke(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axGaNvXHQ-ml",
        "outputId": "e3f8daf5-6729-40ac-9b4e-9c5aad1c33cc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Who won 2011 cricket world cup', 'text': \"\\n\\nThe 2011 Cricket World Cup was held in India, Sri Lanka, and Bangladesh. The final match of the tournament took place on April 2, 2011, between India and Sri Lanka at the Wankhede Stadium in Mumbai, India.\\n\\nIndia won the match by six wickets, with Gautam Gambhir being named the Man of the Match for his outstanding performance. This was India's second Cricket World Cup victory, the first being in 1983.\\n\\nSo, India won the 2011 Cricket World Cup.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2 to access Hugging Face Endpoint is Hugging Face Pipeline\n",
        "# This is used to download the complete hugging face pipeline in the local machine\n",
        "# which will take a huge  amount of space.It cannot be done for all the models\n",
        "# considering limitation of space\n",
        "# But still we can download smaller sized models"
      ],
      "metadata": {
        "id": "HdvthvZgSpeM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
      ],
      "metadata": {
        "id": "BOQoJLtiTPuk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "model  = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "RHmJE8toUH9j"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are various tasks that can be peroformed which include text generation, summarization\n",
        "from transformers import GPT2TokenizerFast\n",
        "pipe = pipeline(\"text-generation\",model = model, tokenizer = tokenizer,max_new_tokens = 100)\n",
        "hf = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "id": "q5YYsX8wVSBQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVuYB04bWDpU",
        "outputId": "357dfadc-21e8-44a7-aa38-89d6405eaa1e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7c5c64dd6320>)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are using text generation this means that the model completes the incomplete\n",
        "# sentence given by us\n",
        "hf.invoke(\"Langchain is a company\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "_uThBYxNW5mq",
        "outputId": "7175a4ee-fdce-4c32-9bd3-745e8bbbfb5c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Langchain is a company based in the UK. At first, it was working on the idea for the company's smartphone. And we wanted to share the fact that it's very fast and not only on the iPhone 5.\\n\\nWe believe in a number of aspects of the phone. Firstly, users can be confident in their smartphones, especially if there seems to be a large crowd on the internet. Secondly, there are people wanting to connect with someone they know by video or e-mail, but also online.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(\"Automation is real\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uffpjm-qXDJ5",
        "outputId": "fb3b86bb-af30-467b-b792-71c6f7c9ee31"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Automation is real. Your company is part of a brand, and you could tell which of your products meets its goals every time you sell something. To this very day the only difference between software that automates your entire job and what you build will tell you how much effort you put into the business. It's like trying to quantify an energy company based on its sales force.\\n\\nWhat makes this idea useful is that you can tell which of those products you would like to automate by what kind of jobs you want\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use HuggingFacePipelines with gpu\n",
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id = \"gpt2\",\n",
        "    task = \"text-generation\",\n",
        "    device = -.0001 ,# replace with device_map = \"auto\" to use the accelerated library\n",
        "    pipeline_kwargs= {\"max_new_tokens\": 100}\n",
        ")"
      ],
      "metadata": {
        "id": "7y7NzoC_Xbgv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "baj4bxpTYjOl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt|gpu_llm"
      ],
      "metadata": {
        "id": "-fxFdE3fY8Mu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is artificaial Intelligence\"\n",
        "chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "S1x9TQ7dZIqm",
        "outputId": "e56803e7-48dd-4f2a-8ffc-4377af4726e2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: What is artificaial Intelligence\\n\\nAnswer: Let\\'s think step by step. To be honest, it does not look very interesting in the real world. When was the last time you heard of \"magic\"? And do you understand that the real magic isn\\'t \"magic\"? Do you even realise that magic has been scientifically invented?\\n\\nAnswer: Absolutely not. In his book, Richard Wright describes his own experiences in his work with computers: \"I was never used to thinking about how machines like computers could do anything more than this. It was all about being able to do'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5nksYgdZXiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}