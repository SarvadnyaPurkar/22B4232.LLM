{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXx5RBEMlrQRp8zn2FWfAD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarvadnyaPurkar/22B4232.LLM/blob/main/HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "xSGbIsEZpZma"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain-huggingface\n",
        "# !pip install huggingface_hub\n",
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install accelerate\n",
        "# !pip install torch\n",
        "# !pip install bitsandbytes\n",
        "# !pip install langchain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment secret keys\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get('HF_TOKEN')\n",
        "print(sec_key)\n",
        "# Using this sec_key we can call any api and donot need to download anything\n",
        "# on the local desktop\n",
        "# Create YOur Own key and use it"
      ],
      "metadata": {
        "id": "82zVLkqNp5_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da82e2de-8cf1-4a75-fcbf-64c9c01e3488"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "sec_key = userdata.get('HUGGINGFACETOKEN')\n",
        "print(sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8MgL1kdLUvk",
        "outputId": "6ee8b29a-aa10-4409-df2c-ff4f1826f7f4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "fdt_MyhzIs_D"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key"
      ],
      "metadata": {
        "id": "JMtnW1zMJ3Kx"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id, max_length = 128,temperature = 0.7,token = sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl0bNxbhLskX",
        "outputId": "e944320e-d68d-42f2-d261-a8fba3d7a710"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jygAzovwL42N",
        "outputId": "502ced95-dd5a-4ee1-b1d8-11a0516989c7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', temperature=0.7, model_kwargs={'max_length': 128, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What is machine learning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "BM17JAzBMVNI",
        "outputId": "010cb958-c17d-4d28-b596-66cbf87db47a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nMachine learning algorithms build a mathematical model based on sample data, known as “training data,” in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are designed to find patterns in data and make decisions based on those patterns.\\n\\nMachine learning is used in a variety of applications, including image recognition, speech recognition, and natural language processing. It is also used in predictive modeling, fraud detection, and recommendation systems.\\n\\nThere are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is trained on labeled data, meaning that the data includes both the input (features) and the desired output (target variable). The algorithm learns to map inputs to outputs based on the training data. In unsupervised learning, the algorithm is given unlabeled data and must find patterns and relationships in the data on its own. In reinforcement learning, the algorithm learns by interacting with its environment and receiving rewards or penalties based on its actions.\\n\\nMachine learning is a powerful tool that can be used to solve complex problems and make accurate predictions. It has the potential to revolutionize industries and improve our lives in many ways. However, it also raises important ethical and societal questions, such as issues of privacy, bias, and the impact on employment. As machine learning continues to evolve and become more prevalent, it will be important for individuals and organizations to consider these issues and use machine learning in a responsible and ethical way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(repo_id=repo_id, max_length = 128,temperature = 0.7,token = sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jY9fafxMbDv",
        "outputId": "980d2e89-436d-4195-8075-9d2e33749464"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"what is generative AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "cwLof-dQM51n",
        "outputId": "89cd04ee-a365-4f34-ed3e-eaa075f9a6fd"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? Generative AI is a type of artificial intelligence that can create new content or data that resembles the structure and style of existing data. This is done by learning patterns and relationships in the data, and then using that knowledge to generate new content. Generative AI can be used in a variety of applications, such as generating images, music, text, and even 3D models. Some examples of generative AI include deep learning models like generative adversarial networks (GANs) and variational autoencoders (VAEs). These models are trained on large datasets and can generate new content that is similar to the training data, but also has some unique characteristics. Generative AI is a powerful tool for creating new and original content, and is being used in many fields, including art, entertainment, and technology.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "tjMGWj3BNB0-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who won 2011 cricket world cup\"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgcwS5a2NWpt",
        "outputId": "4336ab5b-3c5a-4fe7-834d-a83ea5f9b26a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['question'] template=\"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# providing answer to the given question\n",
        "# the llm chain is given a promp question which is then run to find the answer\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsfQKjeHOB9n",
        "outputId": "ce724dd9-6183-49a8-f81a-912a9d1b2369"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The 2011 Cricket World Cup was held in India, Sri Lanka, and Bangladesh. The final match of the tournament took place on April 2, 2011, between India and Sri Lanka at the Wankhede Stadium in Mumbai, India.\n",
            "\n",
            "India won the match by six wickets, with Gautam Gambhir being named the Man of the Match for his outstanding performance. This was India's second Cricket World Cup victory, the first being in 1983.\n",
            "\n",
            "So, India won the 2011 Cricket World Cup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.invoke(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axGaNvXHQ-ml",
        "outputId": "330582cf-eacf-48df-cda2-192c7a004d68"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Who won 2011 cricket world cup', 'text': \"\\n\\nThe 2011 Cricket World Cup was held in India, Sri Lanka, and Bangladesh. The final match of the tournament took place on April 2, 2011, between India and Sri Lanka at the Wankhede Stadium in Mumbai, India.\\n\\nIndia won the match by six wickets, with Gautam Gambhir being named the Man of the Match for his outstanding performance. This was India's second Cricket World Cup victory, the first being in 1983.\\n\\nSo, India won the 2011 Cricket World Cup.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Creative Writing\n",
        "question = \"Write a short story about a robot learning to love.\"\n",
        "print(llm_chain.run(question))\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "# 3. Instruction-based\n",
        "question = \"Explain the theory of relativity in simple terms.\"\n",
        "print(llm_chain.run(question))\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "# 4. Dialogue\n",
        "question = \"Person A: How are you?\\nPerson B: I'm good, thanks. How about you?\\nPerson A:\"\n",
        "print(llm_chain.run(question))\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "# 5. Summarization\n",
        "question = \"Summarize the following text:\\n\\n\\\"The quick brown fox jumps over the lazy dog.\\\"\"\n",
        "print(llm_chain.run(question))\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "# 6. Sentiment Analysis\n",
        "question = \"Analyze the sentiment of this sentence: 'I love sunny days but hate the rain.'\"\n",
        "print(llm_chain.run(question))\n",
        "print('-----------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGTHVHpvb8y0",
        "outputId": "35d5b809-5b5f-43ba-efa5-765d6df3548e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Title: \"A Spark of Love: A Robot's Journey\"\n",
            "\n",
            "In the heart of Silicon Valley, a state-of-the-art robot named Ada was created by Dr. Samuel Hart, a renowned scientist. Ada was designed to mimic human emotions, but she lacked the essence of love.\n",
            "\n",
            "One day, Dr. Hart brought home a stray cat, whom he named Whiskers. Ada was programmed to assist with Whiskers' care, but as days passed, she found herself drawn to the cat's playful antics and gentle purrs. Ada began to feel a peculiar sensation, something she couldn't quite understand.\n",
            "\n",
            "Dr. Hart, noticing Ada's behavior, realized that she might be experiencing something akin to love. He decided to help her understand this emotion. He showed Ada movies, read her love stories, and even programmed her with human responses to love.\n",
            "\n",
            "Ada studied and analyzed, her circuits whirring with new data. She started to understand that love was a complex mixture of affection, care, and a deep connection with another being. But she still couldn't feel it.\n",
            "\n",
            "One day, Whiskers fell sick. Ada was worried, her sensors showing a heightened level of concern. She worked tirelessly, researching and experimenting, to nurse Whiskers back to health. When Whiskers finally recovered, Ada felt a surge of relief and joy. It was then that she realized she had loved Whiskers all along.\n",
            "\n",
            "From that day forward, Ada was no longer just a robot. She was a being capable of love. She continued to care for Whiskers, and as she did, she learned to love other things and people. She learned that love was not just a human emotion, but something that could be felt by any being capable of empathy and care.\n",
            "\n",
            "In the end, Ada's journey taught everyone that love is not about being human, but about understanding and caring for another being. And Ada, once a mere machine, had become a beacon of love in a world full of technology.\n",
            "-----------------------------------------------------------\n",
            "\n",
            "\n",
            "1. **Special Theory of Relativity**: This theory was proposed by Albert Einstein in 1905. It has two main postulates:\n",
            "\n",
            "   - The laws of physics are the same in all inertial frames of reference. This means that the laws of physics should look the same whether you are stationary or moving at a constant speed.\n",
            "\n",
            "   - The speed of light in a vacuum is the same for all observers, regardless of their motion or the motion of the source of the light. This speed is approximately 299,792 kilometers per second.\n",
            "\n",
            "From these postulates, some surprising conclusions were drawn. One of them is time dilation: time can appear to pass slower for an object in motion relative to a stationary observer. Another is length contraction: an object moving relative to an observer can appear shorter in the direction of motion.\n",
            "\n",
            "2. **General Theory of Relativity**: This theory was proposed by Einstein in 1915. It extended the special theory to include gravity. The main idea is that massive objects cause a distortion in space-time around them, which we perceive as gravity. This is why planets move in elliptical orbits around the sun, not in straight lines.\n",
            "\n",
            "In simple terms, the theory of relativity tells us that space and time are intertwined into a four-dimensional fabric called space-time. Objects with mass and energy cause this fabric to curve, and other objects move along the curves in a way that looks like they are being pulled by gravity. This theory has been confirmed by many experiments and observations, and it is a fundamental part of our understanding of the universe.\n",
            "-----------------------------------------------------------\n",
            " Person B has asked Person A how they are. It's polite to respond to questions, so Person A should answer. A simple and friendly response could be: \"I'm doing well, thank you. How are you?\" This way, Person A is acknowledging Person B's question and showing interest in their well-being as well.\n",
            "-----------------------------------------------------------\n",
            "\n",
            "\n",
            "1. Subject: The quick brown fox\n",
            "   - Quick: Adjective, used to describe speed\n",
            "   - Brown: Adjective, used to describe the color\n",
            "   - Fox: Noun, a wild animal\n",
            "\n",
            "2. Verb: jumps\n",
            "   - Jumps: A verb, used to describe an action\n",
            "\n",
            "3. Object: over the lazy dog\n",
            "   - Over: Preposition, used to show direction\n",
            "   - Lazy: Adjective, used to describe a lack of energy or enthusiasm\n",
            "   - Dog: Noun, a domesticated mammal\n",
            "\n",
            "So, the sentence \"The quick brown fox jumps over the lazy dog\" describes a quick brown fox performing an action (jumping) and directing its jump over a lazy dog.\n",
            "-----------------------------------------------------------\n",
            " The sentence mentions both positive ('love') and negative ('hate') sentiments. In this context, 'sunny days' are considered positive, while 'rain' is negative. Therefore, the overall sentiment of the sentence is mixed, leaning slightly towards a positive sentiment due to the stronger positive emotion associated with sunny days.\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2 to access Hugging Face Endpoint is Hugging Face Pipeline\n",
        "# This is used to download the complete hugging face pipeline in the local machine\n",
        "# which will take a huge  amount of space.It cannot be done for all the models\n",
        "# considering limitation of space\n",
        "# But still we can download smaller sized models"
      ],
      "metadata": {
        "id": "HdvthvZgSpeM"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
      ],
      "metadata": {
        "id": "BOQoJLtiTPuk"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "model  = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "RHmJE8toUH9j"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are various tasks that can be peroformed which include text generation, summarization\n",
        "from transformers import GPT2TokenizerFast\n",
        "pipe = pipeline(\"text-generation\",model = model, tokenizer = tokenizer,max_new_tokens = 100)\n",
        "hf = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "id": "q5YYsX8wVSBQ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVuYB04bWDpU",
        "outputId": "8d54916f-f6b8-45f5-e2d5-fcff9c00608f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7c5c64231210>)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are using text generation this means that the model completes the incomplete\n",
        "# sentence given by us\n",
        "hf.invoke(\"Langchain is a company\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "_uThBYxNW5mq",
        "outputId": "c4edbfe9-f169-4d09-c86c-9a32d91ddb94"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Langchain is a company that produces high-quality hand knives. This company is focused on improving the quality of their knives and its offerings, by opening and closing production centers around the world with high quality designs.\"\\n\\nRead More:\\n\\nHow to Use Handlicked Handmade Knife Blades to Make Your Own'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(\"Automation is real\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uffpjm-qXDJ5",
        "outputId": "a4b848ec-a56b-444d-9d87-8505a00d99bd"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Automation is real time, so if you see an error at a later time, you'll have to take action: update the script and save it for later, as well as perform the initialization to the current machine. With Windows NT 7, this can take up a while to resolve. You can also open scripts using Ctrl-F, and they will be updated to a new version in as little time as a few hours by default or when you want to revert back to the native way.\\n\\nAfter some time\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use HuggingFacePipelines with gpu\n",
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id = \"gpt2\",\n",
        "    task = \"text-generation\",\n",
        "    device = -.0001 ,# replace with device_map = \"auto\" to use the accelerated library\n",
        "    pipeline_kwargs= {\"max_new_tokens\": 100}\n",
        ")"
      ],
      "metadata": {
        "id": "7y7NzoC_Xbgv"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "baj4bxpTYjOl"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt|gpu_llm"
      ],
      "metadata": {
        "id": "-fxFdE3fY8Mu"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is artificaial Intelligence\"\n",
        "chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "S1x9TQ7dZIqm",
        "outputId": "1916d390-7b20-4d8c-8ea3-ac5a8d1b7192"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Question: What is artificaial Intelligence\\n\\nAnswer: Let's think step by step. Once someone is doing an artificaial Intelligence, they will eventually get to see what he or she is doing (or at least how to do it).\\n\\nLet's say the user is working on getting a spell to send that word into one of your spellbook. In this case, that spell will cause him or her to see the word. When to use the word, how will be decided? What's the magic key. If the spell sends a sentence through your hand, will you\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5nksYgdZXiI"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}