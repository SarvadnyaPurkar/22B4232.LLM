{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6959b0f8-e7ac-41a8-b8d1-af1658c3de97",
   "metadata": {},
   "source": [
    "1)Data Sources and collections methods\n",
    "There are various data sources from where we can get the required data  \n",
    "i)Open AI's GPT database :- The current GPT models are trained on a higher set of database hence their data is one of the best sources for collection of data  \n",
    "ii)Reddit Datasets:- Extraxted conversations from  Reddit, available on platforms like kaggle  \n",
    "iii)ChatterBot Corpus :- Contains data for training conversational agents.  \n",
    "iv)Cornell Movie Dialogues:- Contains a large number of dialogues from movie scripts.  \n",
    "v)Microsoft's Conversational AI Datasets: Contains various dialogues and tasks.\n",
    "vi)APIs: Use APIs provided by platforms like Twitter, Reddit, and others to collect data.   \n",
    "vii)Web Scraping: For specific needs, you might scrape websites, keeping in mind the legal implications and terms of service.  \n",
    "viii)Public Datasets: Use repositories like Kaggle, UCI Machine Learning Repository, and others.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aeb9cf-6fa0-431d-a95d-a470b3077de6",
   "metadata": {},
   "source": [
    "2)Data Cleaning  \n",
    "i) Handling missing values:- There are various methods of handling missing values  \n",
    "a)We can directly put a constant value in place of null, generally the constant value being the mean of the remaining dataset  \n",
    "b)We should remove the outliers and boxplot is a very good ploting method which helps to identify and remove the outliers  \n",
    "c)We can group the data into different types and then try to find similarities and fill in the null values  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98441a8e-4d82-428b-9cdf-542acd96e822",
   "metadata": {},
   "source": [
    "3)Preprocessing  \n",
    "i) Tokenisation:- Tokenisation is a process in which the paragraphs are converted to tokens(sentences) by dividing them using full stops, sentences are further converted to tokens(words) by dividing themusing spaces.  \n",
    "ii) Remove Stop words:- we can remve stop words like me, myself, yours etc, which donot contribute to the information of the given sentence and are just used for the betterment of the language using nltk's corpus library stopword.  \n",
    "iii) Lemmatization/Stemming:- using Porter Stemmer of nltk we can find the stem of the word which helps to classify the words into them stem words eg stemword for loving and loves is love  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee146162-78c2-4ce9-95b9-8aa79238ee8b",
   "metadata": {},
   "source": [
    "4)Model Training   \n",
    "Select an machine framework and start training  \n",
    "a)TensorFlow  \n",
    "b)Pytorch  \n",
    "c)Hugging face Transformers  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0466d5-5e84-4dd0-8e3f-478d9380392e",
   "metadata": {},
   "source": [
    "5)Pre-trained Models  \n",
    "Starting with pre-trained models can save a lot of time:  \n",
    "\n",
    "GPT-3/GPT-4 (OpenAI)  \n",
    "BERT (Google)  \n",
    "T5 (Google)  \n",
    "DialogGPT (Microsoft)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41826635-831d-45d6-ba05-7ba3d40586e5",
   "metadata": {},
   "source": [
    "6)Fine-Tuning   \n",
    "Fine-tune pre-trained models on your specific dataset to improve performance for your use case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6e415-4682-44b2-8f04-42089084c4de",
   "metadata": {},
   "source": [
    "7)Resources and Tools:  \n",
    "Hugging Face Datasets: A vast collection of ready-to-use datasets for NLP.  \n",
    "Kaggle Datasets: Community datasets that can be used for a variety of AI projects.  \n",
    "GitHub: Many projects have shared datasets and code that can be helpful.7)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d606f0f-b5ef-495e-b830-3a33a122b9a4",
   "metadata": {},
   "source": [
    "Work Flow Summary  \n",
    "Collect Data: Use APIs or download public datasets.  \n",
    "Preprocess Data: Clean and prepare the data for training.  \n",
    "Select Model: Choose a suitable model or pre-trained transformer.  \n",
    "Train/Fine-tune Model: Train the model on your data.  \n",
    "Evaluate Model: Test the modelâ€™s performance and make improvements.  \n",
    "Deploy Bot: Deploy the bot to interact with users.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21ecfa-2b09-4fcc-bb54-5355e10210d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
